# Let Model Run In Chip

ç»è¿‡ä¸€ç•ªçŸ¥è¯†çš„æ´—ç¤¼ï¼Œåˆåˆ°äº†æ¿€åŠ¨äººå¿ƒðŸš€çš„åœ¨è®¾å¤‡ä¸Šè¿è¡Œæ¨¡åž‹çš„æ—¶åˆ»äº†ï¼è¿™ä¸€æ¬¡æˆ‘ä»¬é©¾è½»å°±ç†Ÿï¼Œç”¨ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ä¸€å¸¦è€Œè¿‡å§ï¼š

- [æ¨¡åž‹å‡†å¤‡](#æ¨¡åž‹å‡†å¤‡)
- [æ¨¡åž‹è½¬æ¢åŠæ¨¡æ‹Ÿå™¨ä½¿ç”¨](#æ¨¡åž‹è½¬æ¢åŠæ¨¡æ‹Ÿå™¨ä½¿ç”¨)
- [è®¾å¤‡è¿è¡Œ](#è®¾å¤‡è¿è¡Œ)
  - [åŽå¤„ç†åˆ†æž](#åŽå¤„ç†åˆ†æž)
  - [Demoä»£ç ç¼–å†™](#Demoä»£ç ç¼–å†™)

## æ¨¡åž‹å‡†å¤‡

å‰é¢ç« èŠ‚å·²ç»ç›´æŽ¥ä½¿ç”¨è¿‡äº† *YOLOv5* çš„æ¨¡åž‹çœ‹åˆ°æ•ˆæžœäº†ï¼Œä½†æ˜¯å®ƒå¹¶æ²¡æœ‰ä½¿ç”¨ `torch.jit.trace` å¯¼å‡ºä½œä¸ºç‘žèŠ¯å¾®å¯ä»¥è¯†åˆ«çš„æ¨¡åž‹ï¼Œå› æ­¤ï¼Œè¿™é‡Œä»‹ç»ä¸‹ *YOLOv5* çš„å¯¼å‡ºæ–¹å¼ã€‚

è¿™é‡Œçš„å¯¼å‡ºå’Œå‡†å¤‡å·¥ä½œï¼ŒåŽ‚å•†éƒ½ç»™å‡ºäº† [README.md](https://github.com/rockchip-linux/rknn-toolkit/tree/master/examples/pytorch/yolov5)  å¯ä»¥è¯´éžå¸¸ç®€å•äº†ï¼Œå¦‚ä¸‹å†…å®¹ä¹Ÿæ˜¯æ‘˜é€‰å…¶ä¸­ï¼š

1. ç›´æŽ¥ä½¿ç”¨ *pytorch* æ¨¡åž‹è½¬ä¸º *rknn* æ¨¡åž‹æ—¶ï¼Œéœ€è¦ä¿®æ”¹ `yolov5/models/yolo.py` æ–‡ä»¶çš„åŽå¤„ç†éƒ¨åˆ†ï¼Œå°† `class Detect(nn.Module)` ç±»çš„å­å‡½æ•° `forward` ä¿®æ”¹ä¸ºå¦‚ä¸‹ä»£ç ï¼š

   ```python
   def forward(self, x):
           z = []  # inference output
           for i in range(self.nl):
               x[i] = self.m[i](x[i])  # conv
   
           return x
   ```

   å¦åˆ™ä¼šåœ¨è½¬æ¨¡åž‹æ—¶æŠ¥ä¸€äº›ç®—å­ä¸æ”¯æŒçš„é”™è¯¯ï¼Œè¿™éƒ¨åˆ†å…¶å®žåŽé¢ä¼šä½¿ç”¨æ‰‹åŠ¨å¤„ç†çš„æ–¹å¼è¡¥é½ã€‚

2. ä½¿ç”¨ *YOLOv5* çš„ `export.py` è„šæœ¬è¿›è¡Œæ¨¡åž‹å¯¼å‡ºï¼Œå’Œä¹‹å‰ `torch.jit.trace` çš„æ–¹å¼ä¸€æ ·ï¼Œå¯¼å‡ºçš„æ¨¡åž‹ä¼šåŒ…å«æ•´ä¸ªç½‘ç»œç»“æž„ä¿¡æ¯ï¼š

   ```shell
   PS D:\learn_pytorch\yolov5\yolov5> python export.py --weights yolov5s.pt --img 640 --batch 1 --include torchscript
   export: weights=yolov5s.pt, img_size=[640], batch_size=1, device=cpu, include=['torchscript'], half=False, inplace=False, train=False, optimize=False, dynamic=False, simplify=False, opset=13
   YOLOv5  v5.0-419-gc5360f6 torch 1.10.2 CPU
   
   Fusing layers...
   Model Summary: 224 layers, 7266973 parameters, 0 gradients
   
   PyTorch: starting from yolov5s.pt (14.8 MB)
   
   TorchScript: starting export with torch 1.10.2...
   TorchScript: export success, saved as yolov5s.torchscript.pt (29.4 MB)
   
   Export complete (3.56s)
   Results saved to D:\learn_pytorch\yolov5\yolov5
   Visualize with https://netron.app
   ```

æœ€ç»ˆå¾—åˆ°çš„ `yolov5s.torchscript.pt` å°±æ˜¯å¯ä»¥è½¬ä¸º *rknn* çš„ *YOLOv5* æ¨¡åž‹äº†ã€‚

**æ³¨ï¼š**

1. æœ€å¥½æŒ‰ç…§åŽ‚å•†å»ºè®®ï¼Œä½¿ç”¨ commit id ä¸º c5360f6e7009eb4d05f14d1cc9dae0963e949213 çš„ *YOLOv5* åˆ†æ”¯ï¼Œå¦åˆ™ä¾ç„¶ä¼šæœ‰è½¬æ¨¡åž‹å‡ºé”™çš„æƒ…å†µã€‚

2. ä½¿ç”¨ commit id ä¸º c5360f6e7009eb4d05f14d1cc9dae0963e949213 çš„ *YOLOv5* åˆ†æ”¯ï¼Œå¯èƒ½ `detect.py` ä¼šè¿è¡Œå‡ºç±»ä¼¼å¦‚ä¸‹é”™è¯¯ï¼š

   ```shell
   PS D:\learn_pytorch\yolov5\yolov5> python detect.py --source .\data\images\bus.jpg
   Downloading https://ultralytics.com/assets/Arial.ttf to C:\Users\Administrator\AppData\Roaming\Ultralytics\Arial.ttf...
   100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 755k/755k [00:01<00:00, 628kB/s]
   detect: weights=yolov5s.pt, source=.\data\images\bus.jpg, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_
   nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False
   requirements: numpy>=1.18.5 not found and is required by YOLOv5, attempting auto-update...
   requirements: 'pip install numpy>=1.18.5' skipped (offline)
   YOLOv5  v5.0-419-gc5360f6 torch 1.10.2 CUDA:0 (NVIDIA GeForce GTX 1050 Ti, 4095.6875MB)
   
   Downloading https://github.com/ultralytics/yolov5/releases/download/v6.1/yolov5s.pt to yolov5s.pt...
   100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14.1M/14.1M [00:06<00:00, 2.26MB/s]
   
   Fusing layers... 
   Model Summary: 213 layers, 7225885 parameters, 0 gradients
   D:\ProgramData\Anaconda3\envs\pytorch\lib\site-packages\torch\functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ..\aten\src\ATen\native\
   TensorShape.cpp:2157.)
     return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
   Traceback (most recent call last):
     File "detect.py", line 289, in <module>
       main(opt)
     File "detect.py", line 284, in main
       run(**vars(opt))
     File "D:\ProgramData\Anaconda3\envs\pytorch\lib\site-packages\torch\autograd\grad_mode.py", line 28, in decorate_context
       return func(*args, **kwargs)
       model(torch.zeros(1, 3, *imgsz).to(device).type_as(next(model.parameters())))  # run once
     File "D:\ProgramData\Anaconda3\envs\pytorch\lib\site-packages\torch\nn\modules\module.py", line 1102, in _call_impl
       return forward_call(*input, **kwargs)
     File "D:\learn_pytorch\yolov5\yolov5\models\yolo.py", line 123, in forward
       return self.forward_once(x, profile, visualize)  # single-scale inference, train
     File "D:\learn_pytorch\yolov5\yolov5\models\yolo.py", line 155, in forward_once
       x = m(x)  # run
     File "D:\ProgramData\Anaconda3\envs\pytorch\lib\site-packages\torch\nn\modules\module.py", line 1102, in _call_impl
       return forward_call(*input, **kwargs)
     File "D:\learn_pytorch\yolov5\yolov5\models\yolo.py", line 64, in forward
       y[..., 2:4] = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh
   RuntimeError: The size of tensor a (80) must match the size of tensor b (56) at non-singleton dimension 3
   ```

   åŽŸå› åœ¨äºŽï¼Œcommit id ä¸º c5360f6e7009eb4d05f14d1cc9dae0963e949213 çš„ *YOLOv5* åˆ†æ”¯ä½¿ç”¨çš„æ¨¡åž‹å‚æ•° [yolov5s.pt](https://github.com/ultralytics/yolov5/releases/download/v5.0/yolov5s.pt) æ˜¯ v5.0 ç‰ˆæœ¬çš„ï¼Œæ‰‹åŠ¨ä¸‹è½½è¿™é‡Œé“¾æŽ¥çš„æ­£ç¡®ç‰ˆæœ¬æ”¾å…¥å·¥ç¨‹å³å¯ï¼š

   ```shell
   PS D:\learn_pytorch\yolov5\yolov5> python detect.py --source .\data\images\bus.jpg
   detect: weights=yolov5s.pt, source=.\data\images\bus.jpg, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_
   requirements: numpy>=1.18.5 not found and is required by YOLOv5, attempting auto-update...
   requirements: 'pip install numpy>=1.18.5' skipped (offline)
   YOLOv5  v5.0-419-gc5360f6 torch 1.10.2 CUDA:0 (NVIDIA GeForce GTX 1050 Ti, 4095.6875MB)
   
   Fusing layers...
   Model Summary: 224 layers, 7266973 parameters, 0 gradients
   D:\ProgramData\Anaconda3\envs\pytorch\lib\site-packages\torch\functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ..\aten\src\ATen\native\
   TensorShape.cpp:2157.)
     return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
   image 1/1 D:\learn_pytorch\yolov5\yolov5\data\images\bus.jpg: 640x480 4 persons, 1 bus, 1 fire hydrant, Done. (0.028s)
   Results saved to runs\detect\exp5
   Done. (0.091s)
   ```

## æ¨¡åž‹è½¬æ¢åŠæ¨¡æ‹Ÿå™¨ä½¿ç”¨

è¿™é‡Œæä¾›çš„ `yolov5` å­æ–‡ä»¶å¤¹æ˜¯ä¾æ®å®˜æ–¹çš„ä¿®æ”¹éƒ¨åˆ†è€Œæ¥ï¼Œå¯ä»¥ç›´æŽ¥è¿è¡Œæ¨¡æ‹Ÿå™¨çœ‹å®ƒçš„æ•ˆæžœï¼š

```shell
$ python3 test.py
--> Config model
done
--> Loading model
yolov5s.torchscript.pt ********************
W Pt model version is 1.6(same as you can check through <netron>), but the installed pytorch is 1.10.1+cu102. This may cause the model to fail to load.
done
--> Building model
done
--> Export RKNN model
done
--> Init runtime environment
librknn_runtime version 1.7.1 (bd41dbc build: 2021-10-28 16:15:23 base: 1131)
done
--> Running model
done
--> YOLOv5 post process end
done
class: person, score: 0.99826580286026
box coordinate left,top,right,down: [476.197338283062, 257.57459461688995, 559.819507420063, 517.2954005002975]
class: person, score: 0.9967268705368042
box coordinate left,top,right,down: [111.99989169836044, 233.63885617256165, 218.6885238289833, 528.4154651165009]
class: person, score: 0.978425920009613
box coordinate left,top,right,down: [211.42605847120285, 242.08296704292297, 286.1716588139534, 509.9291789531708]
class: person, score: 0.9698996543884277
box coordinate left,top,right,down: [79.78330028057098, 325.25224447250366, 125.36985218524933, 523.5421891212463]
class: bus , score: 0.9923933744430542
box coordinate left,top,right,down: [82.18040478229523, 135.28777557611465, 561.3988188505173, 444.9854788184166]
```

ä¸ºäº†ç®€ä¾¿ï¼Œè¿™é‡ŒåŒæ ·ä½¿ç”¨äº†å‡ ä¸ªæ³¨é‡Šçš„æ–¹å¼æ¥æ”¾å¼€ `pre_compile` çš„æ¨¡åž‹è½¬æ¢ï¼š

```python
# RKNN_MODEL = 'yolov5s_pre_compile.rknn'
RKNN_MODEL = 'yolov5s.rknn'
...
# QUANTIZE_ON = True
QUANTIZE_ON = False
...
# ret = rknn.build(do_quantization=QUANTIZE_ON, dataset=DATASET, pre_compile=True)
ret = rknn.build(do_quantization=QUANTIZE_ON, dataset=DATASET, pre_compile=False)
...
# exit()
```

ä½¿ç”¨ä¸Šé¢çš„å››ä¸ªæ³¨é‡Šéƒ¨åˆ†æ›¿æ¢ç›¸å…³ä»£ç ï¼ˆé™¤ `exit()` å¤–ï¼‰ï¼Œå°±å¯ä»¥å¾—åˆ°èƒ½å¤Ÿåœ¨è®¾å¤‡ä¸Šè¿è¡Œçš„ `yolov5s_pre_compile.rknn` æ¨¡åž‹äº†ï¼š

```shell
$ python3 test.py
--> Config model
done
--> Loading model
yolov5s.torchscript.pt ********************
W Pt model version is 1.6(same as you can check through <netron>), but the installed pytorch is 1.10.1+cu102. This may cause the model to fail to load.
done
--> Building model
W The RKNN Model generated can not run on simulator when pre_compile is True.
W:tensorflow:From /home/huangkailun/.local/lib/python3.6/site-packages/tensorflow/python/framework/function.py:988: calling Graph.create_op (from tensorflow.python.framework.ops) with compute_shapes is deprecated and will be removed in a future version.
Instructions for updating:
Shapes are always computed; don't use the compute_shapes as it has no effect.
W Rename _dequantize_layer to rknn__dequantize_layer
cc1: warning: command line option â€˜-std=c++11â€™ is valid for C++/ObjC++ but not for C
cc1: warning: command line option â€˜-std=c++11â€™ is valid for C++/ObjC++ but not for C
cc1: warning: command line option â€˜-std=c++11â€™ is valid for C++/ObjC++ but not for C
cc1: warning: command line option â€˜-std=c++11â€™ is valid for C++/ObjC++ but not for C
done
--> Export RKNN model
done
```

## è®¾å¤‡è¿è¡Œ

è®¾å¤‡ä¸Šè¿è¡Œçš„éš¾ç‚¹å…¶å®žæ˜¯å®ƒçš„åŽå¤„ç†éƒ¨åˆ†ï¼Œå‰é¢æ¨¡æ‹Ÿå™¨çš„ä»£ç  `test.py` ä¸­å¯ä»¥çœ‹åˆ°æœ‰ä¸ª `yolov5_post_process` å‡½æ•°ï¼Œè¿™ä¸ªå°±æ˜¯ç½‘ç»œçš„åŽå¤„ç†ï¼ˆæ£€æµ‹è¾“å‡ºï¼‰ï¼Œå› æ­¤é›†æˆæ—¶æ¯”å‰é¢çš„ *logistic* è¿˜æ˜¯å¤æ‚ä¸€äº›ï¼Œè¿™é‡Œä¸å¦¨åˆ†æžä¸€ä¸‹ã€‚

### åŽå¤„ç†åˆ†æž

åœ¨åˆ†æžæ ¸å¿ƒçš„ `yolov5_post_process` å‡½æ•°ä¹‹å‰ï¼Œå®ƒè°ƒç”¨äº† *numpy* è½¬æ¢çš„ä¸€äº›ä»£ç ï¼Œå¦‚ä¸‹ï¼š

```python
    # post process
    input0_data = outputs[0]
    input1_data = outputs[1]
    input2_data = outputs[2]

    input0_data = input0_data.reshape([3,-1]+list(input0_data.shape[-2:]))
    input1_data = input1_data.reshape([3,-1]+list(input1_data.shape[-2:]))
    input2_data = input2_data.reshape([3,-1]+list(input2_data.shape[-2:]))

    input_data = list()
    input_data.append(np.transpose(input0_data, (2, 3, 0, 1)))
    input_data.append(np.transpose(input1_data, (2, 3, 0, 1)))
    input_data.append(np.transpose(input2_data, (2, 3, 0, 1)))
    
    print('--> YOLOv5 post process end')
    boxes, classes, scores = yolov5_post_process(input_data)
    print('done')
```

é€šè¿‡åŠ ä¸€äº›æ‰“å°ï¼Œå‘çŽ°æ¨¡åž‹çš„è¾“å‡ºæ˜¯ `outputs[0]`ï¼Œ`outputs[1]` å’Œ `outputs[2]` ï¼Œç»´åº¦åˆ†åˆ«æ˜¯ `(1, 255, 80, 80)`ï¼Œ`(1, 255, 40, 40)` å’Œ `(1, 255, 20, 20)`ï¼Œé€šè¿‡ `reshape` åŽï¼Œå®ƒä¿è¯å…¶ç¬¬ä¸€ç»´å®‰å…¨å˜æˆ 3ï¼Œæœ€åŽä¸¤ç»´ä¿æŒä¸å˜ï¼Œå› æ­¤ `input0_data`ï¼Œ`input1_data` å’Œ `input2_data` çš„ç»´åº¦ä¸º `(3, 85, 80, 80)`ï¼Œ`(3, 85, 40, 40)` å’Œ `(3, 85, 20, 20)`ï¼Œæœ€åŽå†å°†ç»´åº¦è°ƒæ¢æˆ `(80, 80, 3, 85)`ï¼Œ`(40, 40, 3, 85)` å’Œ `(20, 20, 3, 85)` ä¾æ¬¡è¢«åŠ å…¥äº† `input_data` è¿™ä¸ª `list` ä¸­ï¼Œä¼ å…¥ `yolov5_post_process` å‡½æ•°ã€‚

è¿™äº›ä»£è¡¨ä»€ä¹ˆï¼Ÿå¯ä»¥é€šè¿‡ *YOLO* ä¹‹å‰çš„ç†è®ºçŸ¥è¯†å¾—åˆ°ï¼š

![out](./img-storage/out.png)

å®ƒé»˜è®¤çš„è®­ç»ƒå®Œçš„æ¨¡åž‹ï¼Œæ˜¯åŒ…å« 80 ä¸ªç±»åˆ«ï¼Œè¿™ä»Ž

```python
class Detect(nn.Module):
    stride = None  # strides computed during build
    onnx_dynamic = False  # ONNX export parameter

    def __init__(self, nc=80, anchors=(), ch=(), inplace=True):  # detection layer
        super().__init__()
        self.nc = nc  # number of classes
        self.no = nc + 5  # number of outputs per anchor
        ...
```

`class Detect` çš„æž„é€ å‡½æ•°å¯ä»¥çœ‹å‡ºï¼ŒåŠ ä¸Šæ€»çš„ *score* å’Œ 4 ä¸ªåæ ‡ï¼Œå°±æ˜¯ 85 ä¸ªï¼Œè¿™å°±æ˜¯æœ€åŽä¸€ä¸ªç»´åº¦çš„ä¿¡æ¯ï¼Œå‰é¢çš„ `(80, 80)`ï¼Œ`(40, 40)` å’Œ `(20, 20)` åˆ™ä»£è¡¨ç€å°†å›¾åƒåˆ†å‰²æˆäº†å¤šå°‘ä¸ªå—ï¼Œè€Œç»Ÿä¸€çš„è¿™ä¸ª 3 åˆ™ä»£è¡¨äº†æ¯ä¸ª *grid cell* é¢„æµ‹çš„è¾¹ç•Œæ¡†çš„æ•°é‡ï¼Œå³ä¹‹å‰æåˆ°çš„ï¼ŒåŒ…å« *anchor* çš„ä¸ªæ•°ã€‚

ä¹‹å‰æåˆ°è¿‡ï¼Œ*anchor* æ˜¯é€šè¿‡ *K-means* èšç±»å¾—åˆ°çš„ï¼Œä½†æ˜¯ `test.py` ä¸­ç›´æŽ¥ç»™å‡ºäº†å¸¸æ•°é¡¹

```python
def yolov5_post_process(input_data):
    masks = [[0, 1, 2], [3, 4, 5], [6, 7, 8]]
    anchors = [[10, 13], [16, 30], [33, 23], [30, 61], [62, 45],
              [59, 119], [116, 90], [156, 198], [373, 326]]
    ...
```

æ˜¯å¦æ­£ç¡®å‘¢ï¼Ÿå¯ä»¥é€šè¿‡

```python
class Detect(nn.Module):
    ...
    def forward(self, x):
        z = []  # inference output
        for i in range(self.nl):
            x[i] = self.m[i](x[i])  # conv
            bs, _, ny, nx = x[i].shape  # x(bs,255,20,20) to x(bs,3,20,20,85)
            x[i] = x[i].view(bs, self.na, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()

            if not self.training:  # inference
                if self.grid[i].shape[2:4] != x[i].shape[2:4] or self.onnx_dynamic:
                    self.grid[i] = self._make_grid(nx, ny).to(x[i].device)

                y = x[i].sigmoid()
                print("anchor_grid-----------")
                print(self.anchor_grid[i])
                if self.inplace:
                    y[..., 0:2] = (y[..., 0:2] * 2. - 0.5 + self.grid[i]) * self.stride[i]  # xy
                    y[..., 2:4] = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh
                else:  # for YOLOv5 on AWS Inferentia https://github.com/ultralytics/yolov5/pull/2953
                    xy = (y[..., 0:2] * 2. - 0.5 + self.grid[i]) * self.stride[i]  # xy
                    wh = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i].view(1, self.na, 1, 1, 2)  # wh
                    y = torch.cat((xy, wh, y[..., 4:]), -1)
                z.append(y.view(bs, -1, self.no))

        return x if self.training else (torch.cat(z, 1), x)
```

åœ¨åŽŸå§‹çš„ `class Detect` çš„ `forward` ä¸­åŠ ä¸Šæ‰“å°å°±å¯ä»¥å¾—åˆ° *anchor* çš„å€¼æ¥æ¯”è¾ƒï¼š

```
PS D:\learn_pytorch\yolov5\yolov5> python detect.py --source .\data\images\zidane.jpg
detect: weights=yolov5s.pt, source=.\data\images\zidane.jpg, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnost
ic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False
requirements: numpy>=1.18.5 not found and is required by YOLOv5, attempting auto-update...
requirements: 'pip install numpy>=1.18.5' skipped (offline)
YOLOv5  v5.0-419-gc5360f6 torch 1.10.2 CUDA:0 (NVIDIA GeForce GTX 1050 Ti, 4095.6875MB)

Fusing layers... 
Model Summary: 224 layers, 7266973 parameters, 0 gradients
...
image 1/1 D:\learn_pytorch\yolov5\yolov5\data\images\zidane.jpg: anchor_grid-----------
tensor([[[[[10., 13.]]],


         [[[16., 30.]]],


         [[[33., 23.]]]]], device='cuda:0')
anchor_grid-----------
tensor([[[[[ 30.,  61.]]],


         [[[ 62.,  45.]]],


         [[[ 59., 119.]]]]], device='cuda:0')
anchor_grid-----------
tensor([[[[[116.,  90.]]],


         [[[156., 198.]]],


         [[[373., 326.]]]]], device='cuda:0')
384x640 2 persons, 2 ties, Done. (0.032s)
Results saved to runs\detect\exp7
Done. (0.098s)
```

å‘çŽ°å®Œå…¨ä¸€è‡´ï¼Œå› ä¸ºæ¨¡åž‹å‚æ•°å·²ç»è®­ç»ƒå¥½äº†ï¼Œè‡ªç„¶è‡ªå¸¦çš„ *YOLOv5* æ¨¡åž‹çš„ `anchor` ä¹Ÿæ˜¯è®­ç»ƒå¥½çš„å¸¸æ•°é¡¹ã€‚

å†æŽ¥ç€åˆ†æžä¸‹ä¸€éƒ¨åˆ†ï¼š

```python
def yolov5_post_process(input_data):
    masks = [[0, 1, 2], [3, 4, 5], [6, 7, 8]]
    anchors = [[10, 13], [16, 30], [33, 23], [30, 61], [62, 45],
              [59, 119], [116, 90], [156, 198], [373, 326]]

    boxes, classes, scores = [], [], []
    for input,mask in zip(input_data, masks):
        b, c, s = process(input, mask, anchors)
        b, c, s = filter_boxes(b, c, s)
        boxes.append(b)
        classes.append(c)
        scores.append(s)
    ...
```

è¿™ä¸ª `for` å¾ªçŽ¯æ‹†è§£äº† `input_data` æœ€ç»ˆå¾—åˆ° `boxes` ï¼Œ`classes` å’Œ `scores` ï¼Œè¿™ä¸ª `process` å‡½æ•°å°±æ˜¯é‡ç‚¹äº†ï¼š

```python
def process(input, mask, anchors):

    anchors = [anchors[i] for i in mask]
    grid_h, grid_w = map(int, input.shape[0:2])

    box_confidence = sigmoid(input[..., 4])
    box_confidence = np.expand_dims(box_confidence, axis=-1)

    box_class_probs = sigmoid(input[..., 5:])

    box_xy = sigmoid(input[..., :2])*2 - 0.5

    col = np.tile(np.arange(0, grid_w), grid_h).reshape(-1, grid_w)
    row = np.tile(np.arange(0, grid_h).reshape(-1, 1), grid_w)
    col = col.reshape(grid_h, grid_w, 1, 1).repeat(3, axis=-2)
    row = row.reshape(grid_h, grid_w, 1, 1).repeat(3, axis=-2)
    grid = np.concatenate((col, row), axis=-1)
    box_xy += grid
    box_xy *= (int(IMG_SIZE[1]/grid_h), int(IMG_SIZE[0]/grid_w))

    box_wh = pow(sigmoid(input[..., 2:4])*2, 2)
    box_wh = box_wh * anchors

    box = np.concatenate((box_xy, box_wh), axis=-1)

    return box, box_confidence, box_class_probs
```

è¿™é‡Œçš„è¿‡ç¨‹ä¸»è¦å¦‚ä¸‹ï¼š

1. å¯ä»¥çœ‹åˆ°ï¼Œ*anchor* æ˜¯ä¸åŒå¤§å°çš„ *grid cell* ä½¿ç”¨ä¸‰ä¸ªä¸åŒçš„ *anchor*ï¼Œå¦‚ 80 * 80 ä½¿ç”¨çš„æ˜¯ *[10, 13], [16, 30], [33, 23]* è¿™ä¸‰ä¸ª
2. `grid_h` å’Œ `grid_w` ï¼Œå°±æ˜¯å–å‡ºçš„å‰ä¸¤ç»´ï¼Œå³åœ¨ä¸‰æ¬¡å¾ªçŽ¯ä¸­åˆ†åˆ«ä¸º *(80, 80)*ï¼Œ*(40, 40)*  å’Œ *(20, 20)*
3. `box_confidence` å–å‡ºçš„å°±æ˜¯å‰é¢æ€»çš„ *score* å€¼ï¼Œç»´åº¦ä¸º *(80, 80, 3)* ç„¶åŽä¸ºäº†åŽç»­æ–¹ä¾¿æ“ä½œï¼Œé€šè¿‡ `np.expand_dims` å°†å…¶åœ¨æœ€åŽæ‰©å……äº†ä¸€ä¸ªç»´åº¦ï¼Œå˜ä¸º *(80, 80, 3, 1)*
4. `box_class_probs` å–å‡ºçš„æ˜¯æœ€åŽ 80 ä¸ªåˆ†ç±»çš„ *score*ï¼Œç»´åº¦ä¸º *(80, 80, 3, 80)*
5. é€šè¿‡ä¸€ç³»åˆ— *grid*ï¼Œ*anchor* åˆ°åƒç´ çš„è½¬æ¢ï¼Œæœ€ç»ˆé€šè¿‡ `np.concatenate` æ‹¼æŽ¥äº† *bx, by, bw, bh* å¾—åˆ°çš„ç»´åº¦ä¸º *(80, 80, 3, 4)*

æŽ¥ä¸‹æ¥å†æ¥çœ‹ä¸‹å®Œæ•´çš„ `yolov5_post_process` ï¼š

```python
def yolov5_post_process(input_data):
    masks = [[0, 1, 2], [3, 4, 5], [6, 7, 8]]
    anchors = [[10, 13], [16, 30], [33, 23], [30, 61], [62, 45],
              [59, 119], [116, 90], [156, 198], [373, 326]]

    boxes, classes, scores = [], [], []
    for input,mask in zip(input_data, masks):
        b, c, s = process(input, mask, anchors)
        b, c, s = filter_boxes(b, c, s)
        boxes.append(b)
        classes.append(c)
        scores.append(s)

    boxes = np.concatenate(boxes)
    boxes = xywh2xyxy(boxes)
    classes = np.concatenate(classes)
    scores = np.concatenate(scores)

    nboxes, nclasses, nscores = [], [], []
    for c in set(classes):
        inds = np.where(classes == c)
        b = boxes[inds]
        c = classes[inds]
        s = scores[inds]

        keep = nms_boxes(b, s)

        nboxes.append(b[keep])
        nclasses.append(c[keep])
        nscores.append(s[keep])

    if not nclasses and not nscores:
        return None, None, None

    boxes = np.concatenate(nboxes)
    classes = np.concatenate(nclasses)
    scores = np.concatenate(nscores)

    return boxes, classes, scores
```

åœ¨ `process` åŽåšçš„äº‹æƒ…å…¶å®žå°±ä¸å¤šäº†ï¼š

1. ä½¿ç”¨ `filter_boxes` ï¼š

   ```python
   def filter_boxes(boxes, box_confidences, box_class_probs):
       box_classes = np.argmax(box_class_probs, axis=-1)
       box_class_scores = np.max(box_class_probs, axis=-1)
       pos = np.where(box_confidences[...,0] >= BOX_THRESH)
   
       boxes = boxes[pos]
       classes = box_classes[pos]
       scores = box_class_scores[pos]
   
       return boxes, classes, scores
   ```

   ç”¨ `np.where` æ¥è¿‡æ»¤ *score* å°äºŽ `BOX_THRESH` çš„å„ä¸ªå€¼

2. ä½¿ç”¨ `np.concatenate` å°† `boxes`ï¼Œ`classes`ï¼Œ`scores` å‰é¢ä¸¤ç»´èžåˆï¼Œå¾—åˆ° *(xxx, 3, 4/80/1)* çš„å‘é‡

3. ä»Žå¾—åˆ†æœ€é«˜çš„ `class` å¼€å§‹éåŽ†ï¼Œä½¿ç”¨ `nms_boxes` ï¼ˆ*non-max suppression*ï¼Œéžæžå¤§å€¼æŠ‘åˆ¶ï¼‰ï¼š

   ```python
   def nms_boxes(boxes, scores):
       x = boxes[:, 0]
       y = boxes[:, 1]
       w = boxes[:, 2] - boxes[:, 0]
       h = boxes[:, 3] - boxes[:, 1]
   
       areas = w * h
       order = scores.argsort()[::-1]
   
       keep = []
       while order.size > 0:
           i = order[0]
           keep.append(i)
   
           xx1 = np.maximum(x[i], x[order[1:]])
           yy1 = np.maximum(y[i], y[order[1:]])
           xx2 = np.minimum(x[i] + w[i], x[order[1:]] + w[order[1:]])
           yy2 = np.minimum(y[i] + h[i], y[order[1:]] + h[order[1:]])
   
           w1 = np.maximum(0.0, xx2 - xx1 + 0.00001)
           h1 = np.maximum(0.0, yy2 - yy1 + 0.00001)
           inter = w1 * h1
   
           ovr = inter / (areas[i] + areas[order[1:]] - inter)
           inds = np.where(ovr <= NMS_THRESH)[0]
           order = order[inds + 1]
       keep = np.array(keep)
       return keep
   ```

   è¿‡æ»¤åŒä¸€ç‰©ä½“é‡å¤çš„è¾¹ç•Œæ¡†ï¼Œæ±‚è§£æ–¹æ³•å’Œä¹‹å‰ç†è®ºä¸€è‡´ï¼ŒæŒ‰ç…§å¾—åˆ†æŽ’åºï¼Œä»Žå¾—åˆ†æœ€é«˜çš„å¼€å§‹éåŽ†ï¼Œå¾—åˆ†æœ€é«˜çš„ç›´æŽ¥æ’å…¥ `keep` è¿™ä¸ª `list` ä¸­ï¼Œè€ŒåŽç»­è¦æ’å…¥çš„ï¼Œåˆ™è¦æ»¡è¶³ *IoU* å°äºŽ `NMS_THRESH` è¿™ä¸ªæ¡ä»¶ï¼Œç»§ç»­æ’å…¥

4. å†ä½¿ç”¨ `np.concatenate` åˆå¹¶ä¸€ä¸ªç»´åº¦ï¼Œå¾—åˆ°æœ€ç»ˆç»“æžœ

è¿™æ ·ï¼Œå†æ¥ä½¿ç”¨ *opencv* çš„å‡½æ•°ç»˜åˆ¶è¾¹ç•Œæ¡†ã€ç‰©ä½“ç±»åˆ«åŠ *score* å°±å¾ˆç®€å•äº†ã€‚

### Demoä»£ç ç¼–å†™

