# Let Model Run In Chip

ÁªèËøá‰∏ÄÁï™Áü•ËØÜÁöÑÊ¥óÁ§ºÔºåÂèàÂà∞‰∫ÜÊøÄÂä®‰∫∫ÂøÉüöÄÁöÑÂú®ËÆæÂ§á‰∏äËøêË°åÊ®°ÂûãÁöÑÊó∂Âàª‰∫ÜÔºÅËøô‰∏ÄÊ¨°Êàë‰ª¨È©æËΩªÂ∞±ÁÜüÔºåÁî®‰ª•‰∏ãÂá†‰∏™ÈÉ®ÂàÜ‰∏ÄÂ∏¶ËÄåËøáÂêßÔºö

- [Ê®°ÂûãÂáÜÂ§á](#Ê®°ÂûãÂáÜÂ§á)
- [Ê®°ÂûãËΩ¨Êç¢ÂèäÊ®°ÊãüÂô®‰ΩøÁî®](#Ê®°ÂûãËΩ¨Êç¢ÂèäÊ®°ÊãüÂô®‰ΩøÁî®)
- [ËÆæÂ§áËøêË°å](#ËÆæÂ§áËøêË°å)
  - [ÂêéÂ§ÑÁêÜÂàÜÊûê](#ÂêéÂ§ÑÁêÜÂàÜÊûê)
  - [Demo‰ª£Á†ÅÁºñÂÜô](#Demo‰ª£Á†ÅÁºñÂÜô)

## Ê®°ÂûãÂáÜÂ§á

ÂâçÈù¢Á´†ËäÇÂ∑≤ÁªèÁõ¥Êé•‰ΩøÁî®Ëøá‰∫Ü *YOLOv5* ÁöÑÊ®°ÂûãÁúãÂà∞ÊïàÊûú‰∫ÜÔºå‰ΩÜÊòØÂÆÉÂπ∂Ê≤°Êúâ‰ΩøÁî® `torch.jit.trace` ÂØºÂá∫‰Ωú‰∏∫ÁëûËäØÂæÆÂèØ‰ª•ËØÜÂà´ÁöÑÊ®°ÂûãÔºåÂõ†Ê≠§ÔºåËøôÈáå‰ªãÁªç‰∏ã *YOLOv5* ÁöÑÂØºÂá∫ÊñπÂºè„ÄÇ

ËøôÈáåÁöÑÂØºÂá∫ÂíåÂáÜÂ§áÂ∑•‰ΩúÔºåÂéÇÂïÜÈÉΩÁªôÂá∫‰∫Ü [README.md](https://github.com/rockchip-linux/rknn-toolkit/tree/master/examples/pytorch/yolov5)  ÂèØ‰ª•ËØ¥ÈùûÂ∏∏ÁÆÄÂçï‰∫ÜÔºåÂ¶Ç‰∏ãÂÜÖÂÆπ‰πüÊòØÊëòÈÄâÂÖ∂‰∏≠Ôºö

1. Áõ¥Êé•‰ΩøÁî® *pytorch* Ê®°ÂûãËΩ¨‰∏∫ *rknn* Ê®°ÂûãÊó∂ÔºåÈúÄË¶Å‰øÆÊîπ `yolov5/models/yolo.py` Êñá‰ª∂ÁöÑÂêéÂ§ÑÁêÜÈÉ®ÂàÜÔºåÂ∞Ü `class Detect(nn.Module)` Á±ªÁöÑÂ≠êÂáΩÊï∞ `forward` ‰øÆÊîπ‰∏∫Â¶Ç‰∏ã‰ª£Á†ÅÔºö

   ```python
   def forward(self, x):
           z = []  # inference output
           for i in range(self.nl):
               x[i] = self.m[i](x[i])  # conv
   
           return x
   ```

   Âê¶Âàô‰ºöÂú®ËΩ¨Ê®°ÂûãÊó∂Êä•‰∏Ä‰∫õÁÆóÂ≠ê‰∏çÊîØÊåÅÁöÑÈîôËØØÔºåËøôÈÉ®ÂàÜÂÖ∂ÂÆûÂêéÈù¢‰ºö‰ΩøÁî®ÊâãÂä®Â§ÑÁêÜÁöÑÊñπÂºèË°•ÈΩê„ÄÇ

2. ‰ΩøÁî® *YOLOv5* ÁöÑ `export.py` ËÑöÊú¨ËøõË°åÊ®°ÂûãÂØºÂá∫ÔºåÂíå‰πãÂâç `torch.jit.trace` ÁöÑÊñπÂºè‰∏ÄÊ†∑ÔºåÂØºÂá∫ÁöÑÊ®°Âûã‰ºöÂåÖÂê´Êï¥‰∏™ÁΩëÁªúÁªìÊûÑ‰ø°ÊÅØÔºö

   ```shell
   PS D:\learn_pytorch\yolov5\yolov5> python export.py --weights yolov5s.pt --img 640 --batch 1 --include torchscript
   export: weights=yolov5s.pt, img_size=[640], batch_size=1, device=cpu, include=['torchscript'], half=False, inplace=False, train=False, optimize=False, dynamic=False, simplify=False, opset=13
   YOLOv5  v5.0-419-gc5360f6 torch 1.10.2 CPU
   
   Fusing layers...
   Model Summary: 224 layers, 7266973 parameters, 0 gradients
   
   PyTorch: starting from yolov5s.pt (14.8 MB)
   
   TorchScript: starting export with torch 1.10.2...
   TorchScript: export success, saved as yolov5s.torchscript.pt (29.4 MB)
   
   Export complete (3.56s)
   Results saved to D:\learn_pytorch\yolov5\yolov5
   Visualize with https://netron.app
   ```

ÊúÄÁªàÂæóÂà∞ÁöÑ `yolov5s.torchscript.pt` Â∞±ÊòØÂèØ‰ª•ËΩ¨‰∏∫ *rknn* ÁöÑ *YOLOv5* Ê®°Âûã‰∫Ü„ÄÇ

**Ê≥®Ôºö**

1. ÊúÄÂ•ΩÊåâÁÖßÂéÇÂïÜÂª∫ËÆÆÔºå‰ΩøÁî® commit id ‰∏∫ c5360f6e7009eb4d05f14d1cc9dae0963e949213 ÁöÑ *YOLOv5* ÂàÜÊîØÔºåÂê¶Âàô‰æùÁÑ∂‰ºöÊúâËΩ¨Ê®°ÂûãÂá∫ÈîôÁöÑÊÉÖÂÜµ„ÄÇ

2. ‰ΩøÁî® commit id ‰∏∫ c5360f6e7009eb4d05f14d1cc9dae0963e949213 ÁöÑ *YOLOv5* ÂàÜÊîØÔºåÂèØËÉΩ `detect.py` ‰ºöËøêË°åÂá∫Á±ª‰ººÂ¶Ç‰∏ãÈîôËØØÔºö

   ```shell
   PS D:\learn_pytorch\yolov5\yolov5> python detect.py --source .\data\images\bus.jpg
   Downloading https://ultralytics.com/assets/Arial.ttf to C:\Users\Administrator\AppData\Roaming\Ultralytics\Arial.ttf...
   100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 755k/755k [00:01<00:00, 628kB/s]
   detect: weights=yolov5s.pt, source=.\data\images\bus.jpg, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_
   nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False
   requirements: numpy>=1.18.5 not found and is required by YOLOv5, attempting auto-update...
   requirements: 'pip install numpy>=1.18.5' skipped (offline)
   YOLOv5  v5.0-419-gc5360f6 torch 1.10.2 CUDA:0 (NVIDIA GeForce GTX 1050 Ti, 4095.6875MB)
   
   Downloading https://github.com/ultralytics/yolov5/releases/download/v6.1/yolov5s.pt to yolov5s.pt...
   100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14.1M/14.1M [00:06<00:00, 2.26MB/s]
   
   Fusing layers... 
   Model Summary: 213 layers, 7225885 parameters, 0 gradients
   D:\ProgramData\Anaconda3\envs\pytorch\lib\site-packages\torch\functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ..\aten\src\ATen\native\
   TensorShape.cpp:2157.)
     return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
   Traceback (most recent call last):
     File "detect.py", line 289, in <module>
       main(opt)
     File "detect.py", line 284, in main
       run(**vars(opt))
     File "D:\ProgramData\Anaconda3\envs\pytorch\lib\site-packages\torch\autograd\grad_mode.py", line 28, in decorate_context
       return func(*args, **kwargs)
       model(torch.zeros(1, 3, *imgsz).to(device).type_as(next(model.parameters())))  # run once
     File "D:\ProgramData\Anaconda3\envs\pytorch\lib\site-packages\torch\nn\modules\module.py", line 1102, in _call_impl
       return forward_call(*input, **kwargs)
     File "D:\learn_pytorch\yolov5\yolov5\models\yolo.py", line 123, in forward
       return self.forward_once(x, profile, visualize)  # single-scale inference, train
     File "D:\learn_pytorch\yolov5\yolov5\models\yolo.py", line 155, in forward_once
       x = m(x)  # run
     File "D:\ProgramData\Anaconda3\envs\pytorch\lib\site-packages\torch\nn\modules\module.py", line 1102, in _call_impl
       return forward_call(*input, **kwargs)
     File "D:\learn_pytorch\yolov5\yolov5\models\yolo.py", line 64, in forward
       y[..., 2:4] = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh
   RuntimeError: The size of tensor a (80) must match the size of tensor b (56) at non-singleton dimension 3
   ```

   ÂéüÂõ†Âú®‰∫éÔºåcommit id ‰∏∫ c5360f6e7009eb4d05f14d1cc9dae0963e949213 ÁöÑ *YOLOv5* ÂàÜÊîØ‰ΩøÁî®ÁöÑÊ®°ÂûãÂèÇÊï∞ [yolov5s.pt](https://github.com/ultralytics/yolov5/releases/download/v5.0/yolov5s.pt) ÊòØ v5.0 ÁâàÊú¨ÁöÑÔºåÊâãÂä®‰∏ãËΩΩËøôÈáåÈìæÊé•ÁöÑÊ≠£Á°ÆÁâàÊú¨ÊîæÂÖ•Â∑•Á®ãÂç≥ÂèØÔºö

   ```shell
   PS D:\learn_pytorch\yolov5\yolov5> python detect.py --source .\data\images\bus.jpg
   detect: weights=yolov5s.pt, source=.\data\images\bus.jpg, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_
   requirements: numpy>=1.18.5 not found and is required by YOLOv5, attempting auto-update...
   requirements: 'pip install numpy>=1.18.5' skipped (offline)
   YOLOv5  v5.0-419-gc5360f6 torch 1.10.2 CUDA:0 (NVIDIA GeForce GTX 1050 Ti, 4095.6875MB)
   
   Fusing layers...
   Model Summary: 224 layers, 7266973 parameters, 0 gradients
   D:\ProgramData\Anaconda3\envs\pytorch\lib\site-packages\torch\functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ..\aten\src\ATen\native\
   TensorShape.cpp:2157.)
     return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
   image 1/1 D:\learn_pytorch\yolov5\yolov5\data\images\bus.jpg: 640x480 4 persons, 1 bus, 1 fire hydrant, Done. (0.028s)
   Results saved to runs\detect\exp5
   Done. (0.091s)
   ```

## Ê®°ÂûãËΩ¨Êç¢ÂèäÊ®°ÊãüÂô®‰ΩøÁî®

ËøôÈáåÊèê‰æõÁöÑ `yolov5` Â≠êÊñá‰ª∂Â§πÊòØ‰æùÊçÆÂÆòÊñπÁöÑ‰øÆÊîπÈÉ®ÂàÜËÄåÊù•ÔºåÂèØ‰ª•Áõ¥Êé•ËøêË°åÊ®°ÊãüÂô®ÁúãÂÆÉÁöÑÊïàÊûúÔºö

```shell
$ python3 test.py
--> Config model
done
--> Loading model
yolov5s.torchscript.pt ********************
W Pt model version is 1.6(same as you can check through <netron>), but the installed pytorch is 1.10.1+cu102. This may cause the model to fail to load.
done
--> Building model
done
--> Export RKNN model
done
--> Init runtime environment
librknn_runtime version 1.7.1 (bd41dbc build: 2021-10-28 16:15:23 base: 1131)
done
--> Running model
done
--> YOLOv5 post process end
done
class: person, score: 0.99826580286026
box coordinate left,top,right,down: [476.197338283062, 257.57459461688995, 559.819507420063, 517.2954005002975]
class: person, score: 0.9967268705368042
box coordinate left,top,right,down: [111.99989169836044, 233.63885617256165, 218.6885238289833, 528.4154651165009]
class: person, score: 0.978425920009613
box coordinate left,top,right,down: [211.42605847120285, 242.08296704292297, 286.1716588139534, 509.9291789531708]
class: person, score: 0.9698996543884277
box coordinate left,top,right,down: [79.78330028057098, 325.25224447250366, 125.36985218524933, 523.5421891212463]
class: bus , score: 0.9923933744430542
box coordinate left,top,right,down: [82.18040478229523, 135.28777557611465, 561.3988188505173, 444.9854788184166]
```

‰∏∫‰∫ÜÁÆÄ‰æøÔºåËøôÈáåÂêåÊ†∑‰ΩøÁî®‰∫ÜÂá†‰∏™Ê≥®ÈáäÁöÑÊñπÂºèÊù•ÊîæÂºÄ `pre_compile` ÁöÑÊ®°ÂûãËΩ¨Êç¢Ôºö

```python
# RKNN_MODEL = 'yolov5s_pre_compile.rknn'
RKNN_MODEL = 'yolov5s.rknn'
...
# QUANTIZE_ON = True
QUANTIZE_ON = False
...
# ret = rknn.build(do_quantization=QUANTIZE_ON, dataset=DATASET, pre_compile=True)
ret = rknn.build(do_quantization=QUANTIZE_ON, dataset=DATASET, pre_compile=False)
...
# exit()
```

‰ΩøÁî®‰∏äÈù¢ÁöÑÂõõ‰∏™Ê≥®ÈáäÈÉ®ÂàÜÊõøÊç¢Áõ∏ÂÖ≥‰ª£Á†ÅÔºàÈô§ `exit()` Â§ñÔºâÔºåÂ∞±ÂèØ‰ª•ÂæóÂà∞ËÉΩÂ§üÂú®ËÆæÂ§á‰∏äËøêË°åÁöÑ `yolov5s_pre_compile.rknn` Ê®°Âûã‰∫ÜÔºö

```shell
$ python3 test.py
--> Config model
done
--> Loading model
yolov5s.torchscript.pt ********************
W Pt model version is 1.6(same as you can check through <netron>), but the installed pytorch is 1.10.1+cu102. This may cause the model to fail to load.
done
--> Building model
W The RKNN Model generated can not run on simulator when pre_compile is True.
W:tensorflow:From /home/huangkailun/.local/lib/python3.6/site-packages/tensorflow/python/framework/function.py:988: calling Graph.create_op (from tensorflow.python.framework.ops) with compute_shapes is deprecated and will be removed in a future version.
Instructions for updating:
Shapes are always computed; don't use the compute_shapes as it has no effect.
W Rename _dequantize_layer to rknn__dequantize_layer
cc1: warning: command line option ‚Äò-std=c++11‚Äô is valid for C++/ObjC++ but not for C
cc1: warning: command line option ‚Äò-std=c++11‚Äô is valid for C++/ObjC++ but not for C
cc1: warning: command line option ‚Äò-std=c++11‚Äô is valid for C++/ObjC++ but not for C
cc1: warning: command line option ‚Äò-std=c++11‚Äô is valid for C++/ObjC++ but not for C
done
--> Export RKNN model
done
```

## ËÆæÂ§áËøêË°å

ËÆæÂ§á‰∏äËøêË°åÁöÑÈöæÁÇπÂÖ∂ÂÆûÊòØÂÆÉÁöÑÂêéÂ§ÑÁêÜÈÉ®ÂàÜÔºåÂâçÈù¢Ê®°ÊãüÂô®ÁöÑ‰ª£Á†Å `test.py` ‰∏≠ÂèØ‰ª•ÁúãÂà∞Êúâ‰∏™ `yolov5_post_process` ÂáΩÊï∞ÔºåËøô‰∏™Â∞±ÊòØÁΩëÁªúÁöÑÂêéÂ§ÑÁêÜÔºàÊ£ÄÊµãËæìÂá∫ÔºâÔºåÂõ†Ê≠§ÈõÜÊàêÊó∂ÊØîÂâçÈù¢ÁöÑ *logistic* ËøòÊòØÂ§çÊùÇ‰∏Ä‰∫õÔºåËøôÈáå‰∏çÂ¶®ÂàÜÊûê‰∏Ä‰∏ã„ÄÇ

### ÂêéÂ§ÑÁêÜÂàÜÊûê

Âú®ÂàÜÊûêÊ†∏ÂøÉÁöÑ `yolov5_post_process` ÂáΩÊï∞‰πãÂâçÔºåÂÆÉË∞ÉÁî®‰∫Ü *numpy* ËΩ¨Êç¢ÁöÑ‰∏Ä‰∫õ‰ª£Á†ÅÔºåÂ¶Ç‰∏ãÔºö

```python
    # post process
    input0_data = outputs[0]
    input1_data = outputs[1]
    input2_data = outputs[2]

    input0_data = input0_data.reshape([3,-1]+list(input0_data.shape[-2:]))
    input1_data = input1_data.reshape([3,-1]+list(input1_data.shape[-2:]))
    input2_data = input2_data.reshape([3,-1]+list(input2_data.shape[-2:]))

    input_data = list()
    input_data.append(np.transpose(input0_data, (2, 3, 0, 1)))
    input_data.append(np.transpose(input1_data, (2, 3, 0, 1)))
    input_data.append(np.transpose(input2_data, (2, 3, 0, 1)))
    
    print('--> YOLOv5 post process end')
    boxes, classes, scores = yolov5_post_process(input_data)
    print('done')
```

ÈÄöËøáÂä†‰∏Ä‰∫õÊâìÂç∞ÔºåÂèëÁé∞Ê®°ÂûãÁöÑËæìÂá∫ÊòØ `outputs[0]`Ôºå`outputs[1]` Âíå `outputs[2]` ÔºåÁª¥Â∫¶ÂàÜÂà´ÊòØ `(1, 255, 80, 80)`Ôºå`(1, 255, 40, 40)` Âíå `(1, 255, 20, 20)`ÔºåÈÄöËøá `reshape` ÂêéÔºåÂÆÉ‰øùËØÅÂÖ∂Á¨¨‰∏ÄÁª¥ÂÆâÂÖ®ÂèòÊàê 3ÔºåÊúÄÂêé‰∏§Áª¥‰øùÊåÅ‰∏çÂèòÔºåÂõ†Ê≠§ `input0_data`Ôºå`input1_data` Âíå `input2_data` ÁöÑÁª¥Â∫¶‰∏∫ `(3, 85, 80, 80)`Ôºå`(3, 85, 40, 40)` Âíå `(3, 85, 20, 20)`ÔºåÊúÄÂêéÂÜçÂ∞ÜÁª¥Â∫¶Ë∞ÉÊç¢Êàê `(80, 80, 3, 85)`Ôºå`(40, 40, 3, 85)` Âíå `(20, 20, 3, 85)` ‰æùÊ¨°Ë¢´Âä†ÂÖ•‰∫Ü `input_data` Ëøô‰∏™ `list` ‰∏≠Ôºå‰º†ÂÖ• `yolov5_post_process` ÂáΩÊï∞„ÄÇ

Ëøô‰∫õ‰ª£Ë°®‰ªÄ‰πàÔºüÂèØ‰ª•ÈÄöËøá *YOLO* ‰πãÂâçÁöÑÁêÜËÆ∫Áü•ËØÜÂæóÂà∞Ôºö

![out](./img-storage/out.png)

ÂÆÉÈªòËÆ§ÁöÑËÆ≠ÁªÉÂÆåÁöÑÊ®°ÂûãÔºåÊòØÂåÖÂê´ 80 ‰∏™Á±ªÂà´ÔºåËøô‰ªé

```python
class Detect(nn.Module):
    stride = None  # strides computed during build
    onnx_dynamic = False  # ONNX export parameter

    def __init__(self, nc=80, anchors=(), ch=(), inplace=True):  # detection layer
        super().__init__()
        self.nc = nc  # number of classes
        self.no = nc + 5  # number of outputs per anchor
        ...
```

`class Detect` ÁöÑÊûÑÈÄ†ÂáΩÊï∞ÂèØ‰ª•ÁúãÂá∫ÔºåÂä†‰∏äÊÄªÁöÑ *score* Âíå 4 ‰∏™ÂùêÊ†áÔºåÂ∞±ÊòØ 85 ‰∏™ÔºåËøôÂ∞±ÊòØÊúÄÂêé‰∏Ä‰∏™Áª¥Â∫¶ÁöÑ‰ø°ÊÅØÔºåÂâçÈù¢ÁöÑ `(80, 80)`Ôºå`(40, 40)` Âíå `(20, 20)` Âàô‰ª£Ë°®ÁùÄÂ∞ÜÂõæÂÉèÂàÜÂâ≤Êàê‰∫ÜÂ§öÂ∞ë‰∏™ÂùóÔºåËÄåÁªü‰∏ÄÁöÑËøô‰∏™ 3 Âàô‰ª£Ë°®‰∫ÜÊØè‰∏™ *grid cell* È¢ÑÊµãÁöÑËæπÁïåÊ°ÜÁöÑÊï∞ÈáèÔºåÂç≥‰πãÂâçÊèêÂà∞ÁöÑÔºåÂåÖÂê´ *anchor* ÁöÑ‰∏™Êï∞„ÄÇ

‰πãÂâçÊèêÂà∞ËøáÔºå*anchor* ÊòØÈÄöËøá *K-means* ËÅöÁ±ªÂæóÂà∞ÁöÑÔºå‰ΩÜÊòØ `test.py` ‰∏≠Áõ¥Êé•ÁªôÂá∫‰∫ÜÂ∏∏Êï∞È°π

```python
def yolov5_post_process(input_data):
    masks = [[0, 1, 2], [3, 4, 5], [6, 7, 8]]
    anchors = [[10, 13], [16, 30], [33, 23], [30, 61], [62, 45],
              [59, 119], [116, 90], [156, 198], [373, 326]]
    ...
```

ÊòØÂê¶Ê≠£Á°ÆÂë¢ÔºüÂèØ‰ª•ÈÄöËøá

```python
class Detect(nn.Module):
    ...
    def forward(self, x):
        z = []  # inference output
        for i in range(self.nl):
            x[i] = self.m[i](x[i])  # conv
            bs, _, ny, nx = x[i].shape  # x(bs,255,20,20) to x(bs,3,20,20,85)
            x[i] = x[i].view(bs, self.na, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()

            if not self.training:  # inference
                if self.grid[i].shape[2:4] != x[i].shape[2:4] or self.onnx_dynamic:
                    self.grid[i] = self._make_grid(nx, ny).to(x[i].device)

                y = x[i].sigmoid()
                print("anchor_grid-----------")
                print(self.anchor_grid[i])
                if self.inplace:
                    y[..., 0:2] = (y[..., 0:2] * 2. - 0.5 + self.grid[i]) * self.stride[i]  # xy
                    y[..., 2:4] = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh
                else:  # for YOLOv5 on AWS Inferentia https://github.com/ultralytics/yolov5/pull/2953
                    xy = (y[..., 0:2] * 2. - 0.5 + self.grid[i]) * self.stride[i]  # xy
                    wh = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i].view(1, self.na, 1, 1, 2)  # wh
                    y = torch.cat((xy, wh, y[..., 4:]), -1)
                z.append(y.view(bs, -1, self.no))

        return x if self.training else (torch.cat(z, 1), x)
```

Âú®ÂéüÂßãÁöÑ `class Detect` ÁöÑ `forward` ‰∏≠Âä†‰∏äÊâìÂç∞Â∞±ÂèØ‰ª•ÂæóÂà∞ *anchor* ÁöÑÂÄºÊù•ÊØîËæÉÔºö

```
PS D:\learn_pytorch\yolov5\yolov5> python detect.py --source .\data\images\zidane.jpg
detect: weights=yolov5s.pt, source=.\data\images\zidane.jpg, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnost
ic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False
requirements: numpy>=1.18.5 not found and is required by YOLOv5, attempting auto-update...
requirements: 'pip install numpy>=1.18.5' skipped (offline)
YOLOv5  v5.0-419-gc5360f6 torch 1.10.2 CUDA:0 (NVIDIA GeForce GTX 1050 Ti, 4095.6875MB)

Fusing layers... 
Model Summary: 224 layers, 7266973 parameters, 0 gradients
...
image 1/1 D:\learn_pytorch\yolov5\yolov5\data\images\zidane.jpg: anchor_grid-----------
tensor([[[[[10., 13.]]],


         [[[16., 30.]]],


         [[[33., 23.]]]]], device='cuda:0')
anchor_grid-----------
tensor([[[[[ 30.,  61.]]],


         [[[ 62.,  45.]]],


         [[[ 59., 119.]]]]], device='cuda:0')
anchor_grid-----------
tensor([[[[[116.,  90.]]],


         [[[156., 198.]]],


         [[[373., 326.]]]]], device='cuda:0')
384x640 2 persons, 2 ties, Done. (0.032s)
Results saved to runs\detect\exp7
Done. (0.098s)
```

ÂèëÁé∞ÂÆåÂÖ®‰∏ÄËá¥ÔºåÂõ†‰∏∫Ê®°ÂûãÂèÇÊï∞Â∑≤ÁªèËÆ≠ÁªÉÂ•Ω‰∫ÜÔºåËá™ÁÑ∂Ëá™Â∏¶ÁöÑ *YOLOv5* Ê®°ÂûãÁöÑ `anchor` ‰πüÊòØËÆ≠ÁªÉÂ•ΩÁöÑÂ∏∏Êï∞È°π„ÄÇ

ÂÜçÊé•ÁùÄÂàÜÊûê‰∏ã‰∏ÄÈÉ®ÂàÜÔºö

```python
def yolov5_post_process(input_data):
    masks = [[0, 1, 2], [3, 4, 5], [6, 7, 8]]
    anchors = [[10, 13], [16, 30], [33, 23], [30, 61], [62, 45],
              [59, 119], [116, 90], [156, 198], [373, 326]]

    boxes, classes, scores = [], [], []
    for input,mask in zip(input_data, masks):
        b, c, s = process(input, mask, anchors)
        b, c, s = filter_boxes(b, c, s)
        boxes.append(b)
        classes.append(c)
        scores.append(s)
    ...
```

Ëøô‰∏™ `for` Âæ™ÁéØÊãÜËß£‰∫Ü `input_data` ÊúÄÁªàÂæóÂà∞ `boxes` Ôºå`classes` Âíå `scores` ÔºåËøô‰∏™ `process` ÂáΩÊï∞Â∞±ÊòØÈáçÁÇπ‰∫ÜÔºö

```python
def process(input, mask, anchors):

    anchors = [anchors[i] for i in mask]
    grid_h, grid_w = map(int, input.shape[0:2])

    box_confidence = sigmoid(input[..., 4])
    box_confidence = np.expand_dims(box_confidence, axis=-1)

    box_class_probs = sigmoid(input[..., 5:])

    box_xy = sigmoid(input[..., :2])*2 - 0.5

    col = np.tile(np.arange(0, grid_w), grid_h).reshape(-1, grid_w)
    row = np.tile(np.arange(0, grid_h).reshape(-1, 1), grid_w)
    col = col.reshape(grid_h, grid_w, 1, 1).repeat(3, axis=-2)
    row = row.reshape(grid_h, grid_w, 1, 1).repeat(3, axis=-2)
    grid = np.concatenate((col, row), axis=-1)
    box_xy += grid
    box_xy *= (int(IMG_SIZE[1]/grid_h), int(IMG_SIZE[0]/grid_w))

    box_wh = pow(sigmoid(input[..., 2:4])*2, 2)
    box_wh = box_wh * anchors

    box = np.concatenate((box_xy, box_wh), axis=-1)

    return box, box_confidence, box_class_probs
```

ËøôÈáåÁöÑËøáÁ®ã‰∏ªË¶ÅÂ¶Ç‰∏ãÔºö

1. ÂèØ‰ª•ÁúãÂà∞Ôºå*anchor* ÊòØ‰∏çÂêåÂ§ßÂ∞èÁöÑ *grid cell* ‰ΩøÁî®‰∏â‰∏™‰∏çÂêåÁöÑ *anchor*ÔºåÂ¶Ç 80 * 80 ‰ΩøÁî®ÁöÑÊòØ *[10, 13], [16, 30], [33, 23]* Ëøô‰∏â‰∏™
2. `grid_h` Âíå `grid_w` ÔºåÂ∞±ÊòØÂèñÂá∫ÁöÑÂâç‰∏§Áª¥ÔºåÂç≥Âú®‰∏âÊ¨°Âæ™ÁéØ‰∏≠ÂàÜÂà´‰∏∫ *(80, 80)*Ôºå*(40, 40)*  Âíå *(20, 20)*
3. `box_confidence` ÂèñÂá∫ÁöÑÂ∞±ÊòØÂâçÈù¢ÊÄªÁöÑ *score* ÂÄºÔºåÁª¥Â∫¶‰∏∫ *(80, 80, 3)* ÁÑ∂Âêé‰∏∫‰∫ÜÂêéÁª≠Êñπ‰æøÊìç‰ΩúÔºåÈÄöËøá `np.expand_dims` Â∞ÜÂÖ∂Âú®ÊúÄÂêéÊâ©ÂÖÖ‰∫Ü‰∏Ä‰∏™Áª¥Â∫¶ÔºåÂèò‰∏∫ *(80, 80, 3, 1)*
4. `box_class_probs` ÂèñÂá∫ÁöÑÊòØÊúÄÂêé 80 ‰∏™ÂàÜÁ±ªÁöÑ *score*ÔºåÁª¥Â∫¶‰∏∫ *(80, 80, 3, 80)*
5. ÈÄöËøá‰∏ÄÁ≥ªÂàó *grid*Ôºå*anchor* Âà∞ÂÉèÁ¥†ÁöÑËΩ¨Êç¢ÔºåÊúÄÁªàÈÄöËøá `np.concatenate` ÊãºÊé•‰∫Ü *bx, by, bw, bh* ÂæóÂà∞ÁöÑÁª¥Â∫¶‰∏∫ *(80, 80, 3, 4)*

Êé•‰∏ãÊù•ÂÜçÊù•Áúã‰∏ãÂÆåÊï¥ÁöÑ `yolov5_post_process` Ôºö

```python
def yolov5_post_process(input_data):
    masks = [[0, 1, 2], [3, 4, 5], [6, 7, 8]]
    anchors = [[10, 13], [16, 30], [33, 23], [30, 61], [62, 45],
              [59, 119], [116, 90], [156, 198], [373, 326]]

    boxes, classes, scores = [], [], []
    for input,mask in zip(input_data, masks):
        b, c, s = process(input, mask, anchors)
        b, c, s = filter_boxes(b, c, s)
        boxes.append(b)
        classes.append(c)
        scores.append(s)

    boxes = np.concatenate(boxes)
    boxes = xywh2xyxy(boxes)
    classes = np.concatenate(classes)
    scores = np.concatenate(scores)

    nboxes, nclasses, nscores = [], [], []
    for c in set(classes):
        inds = np.where(classes == c)
        b = boxes[inds]
        c = classes[inds]
        s = scores[inds]

        keep = nms_boxes(b, s)

        nboxes.append(b[keep])
        nclasses.append(c[keep])
        nscores.append(s[keep])

    if not nclasses and not nscores:
        return None, None, None

    boxes = np.concatenate(nboxes)
    classes = np.concatenate(nclasses)
    scores = np.concatenate(nscores)

    return boxes, classes, scores
```

Âú® `process` ÂêéÂÅöÁöÑ‰∫ãÊÉÖÂÖ∂ÂÆûÂ∞±‰∏çÂ§ö‰∫ÜÔºö

1. ‰ΩøÁî® `filter_boxes` Ôºö

   ```python
   def filter_boxes(boxes, box_confidences, box_class_probs):
       box_classes = np.argmax(box_class_probs, axis=-1)
       box_class_scores = np.max(box_class_probs, axis=-1)
       pos = np.where(box_confidences[...,0] >= BOX_THRESH)
   
       boxes = boxes[pos]
       classes = box_classes[pos]
       scores = box_class_scores[pos]
   
       return boxes, classes, scores
   ```

   Áî® `np.where` Êù•ËøáÊª§ *score* Â∞è‰∫é `BOX_THRESH` ÁöÑÂêÑ‰∏™ÂÄº

2. ‰ΩøÁî® `np.concatenate` Â∞Ü `boxes`Ôºå`classes`Ôºå`scores` ÂâçÈù¢‰∏§Áª¥ËûçÂêàÔºåÂæóÂà∞ *(xxx, 3, 4/80/1)* ÁöÑÂêëÈáè

3. ‰ªéÂæóÂàÜÊúÄÈ´òÁöÑ `class` ÂºÄÂßãÈÅçÂéÜÔºå‰ΩøÁî® `nms_boxes` Ôºà*non-max suppression*ÔºåÈùûÊûÅÂ§ßÂÄºÊäëÂà∂ÔºâÔºö

   ```python
   def nms_boxes(boxes, scores):
       x = boxes[:, 0]
       y = boxes[:, 1]
       w = boxes[:, 2] - boxes[:, 0]
       h = boxes[:, 3] - boxes[:, 1]
   
       areas = w * h
       order = scores.argsort()[::-1]
   
       keep = []
       while order.size > 0:
           i = order[0]
           keep.append(i)
   
           xx1 = np.maximum(x[i], x[order[1:]])
           yy1 = np.maximum(y[i], y[order[1:]])
           xx2 = np.minimum(x[i] + w[i], x[order[1:]] + w[order[1:]])
           yy2 = np.minimum(y[i] + h[i], y[order[1:]] + h[order[1:]])
   
           w1 = np.maximum(0.0, xx2 - xx1 + 0.00001)
           h1 = np.maximum(0.0, yy2 - yy1 + 0.00001)
           inter = w1 * h1
   
           ovr = inter / (areas[i] + areas[order[1:]] - inter)
           inds = np.where(ovr <= NMS_THRESH)[0]
           order = order[inds + 1]
       keep = np.array(keep)
       return keep
   ```

   ËøáÊª§Âêå‰∏ÄÁâ©‰ΩìÈáçÂ§çÁöÑËæπÁïåÊ°ÜÔºåÊ±ÇËß£ÊñπÊ≥ïÂíå‰πãÂâçÁêÜËÆ∫‰∏ÄËá¥ÔºåÊåâÁÖßÂæóÂàÜÊéíÂ∫èÔºå‰ªéÂæóÂàÜÊúÄÈ´òÁöÑÂºÄÂßãÈÅçÂéÜÔºåÂæóÂàÜÊúÄÈ´òÁöÑÁõ¥Êé•ÊèíÂÖ• `keep` Ëøô‰∏™ `list` ‰∏≠ÔºåËÄåÂêéÁª≠Ë¶ÅÊèíÂÖ•ÁöÑÔºåÂàôË¶ÅÊª°Ë∂≥ *IoU* Â∞è‰∫é `NMS_THRESH` Ëøô‰∏™Êù°‰ª∂ÔºåÁªßÁª≠ÊèíÂÖ•

4. ÂÜç‰ΩøÁî® `np.concatenate` ÂêàÂπ∂‰∏Ä‰∏™Áª¥Â∫¶ÔºåÂæóÂà∞ÊúÄÁªàÁªìÊûú

ËøôÊ†∑ÔºåÂÜçÊù•‰ΩøÁî® *opencv* ÁöÑÂáΩÊï∞ÁªòÂà∂ËæπÁïåÊ°Ü„ÄÅÁâ©‰ΩìÁ±ªÂà´Âèä *score* Â∞±ÂæàÁÆÄÂçï‰∫Ü„ÄÇ

### Demo‰ª£Á†ÅÁºñÂÜô

